# Working with Custom Rigs

This is the general workflow for training and inference on custom rigs:

1. convert your input animations into BVH
2. convert BVH into numpy arrays
3. process the numpy arrays to get rot6d vectors with absolute-root values
4. annotate the text describing the animations
5. modify script parameters to include this custom rig
6. train the system
7. do the inference
8. convert the output numpy arrays back to BVH

The "HumanML3D" workflow does step 3, then 6 and 7.


## Introduction

The [Flexible Motion In-Betweening][condmdi] model is trained on the [HumanML3D dataset][hml3d_fork],
originally by [Eric Guo][hml3d_orig], which is a combination of various motion-capture sequences, all
using the SMPL+ 22-node data structure. In order to train on a custom rig, we must specify the joints
of the rig, and edit where the assumptions are made in the training script.

This is the original workflow to obtain the HumanML3D dataset, summarized from the README there:


## Original Workflow for HumanML3D:

1. Download the various datasets from [AMASS][amass] then unzip them into the `amass_data/` folder in
   the HumanML3D repository. Next, download `SMPL+H` models from [MANO][mano] and `DMPLS` models from
   the [SMPL][smpl] sites. Unzip these and put them in the `body_models/` folder. Each of these sites
   requires an account to be created before you download anything. 
2. Run `raw_pose_preprocess.ipynb` on the data. This gets poses from the AMASS data.
3. Run the absolute value versions of `motion_processing.ipynb` and `cal_mean_variance.ipynb`. If you
   cloned the [original][hml3d_orig] repo, please copy the notebooks from the `HumanML3D_abs/` folder
   in [CondMDI][condmdi] to the root of the HumanML3D repo, then run those. In the [fork][hml3d_fork]
   the notebooks are the absolute root joint versions; the original notebooks have the prefix `rel_`.
4. Copy the processed data directory `HumanML3D/` into `dataset/`. The sequence data can now be found
   in `new_joints_abs_3d/`, with the converted data in `new_joint_vecs_abs_3d/`.

[amass]:        https://amass.is.tue.mpg.de/download.php
[smpl]:         https://smpl.is.tue.mpg.de/download.php
[mano]:         https://mano.is.tue.mpg.de/download.php
[condmdi]:      https://github.com/icedwater/CondMDI
[hml3d_fork]:   https://github.com/icedwater/HumanML3D
[hml3d_orig]:   https://github.com/EricGuo5513/HumanML3D


## Preparing a custom dataset for training

Make sure a corresponding set of `$DATASET/joints` and `$DATASET/vecs` is present.
The dimensions of each sequence nd-array in `joints` should be F x J x 3, F is the
number of frames, J the number of joints in the rig, and 3 the coordinates of each
joint. The `vecs` arrays should have dimensions F x (12J - 1) as per Appendix A of
the [paper][condpaper]. These are generated by `motion_processing.ipynb` in normal
operation with `HumanML3D`, but if we have the joints already, we only need to run
`build_vectors.py` which constructs the `vecs` arrays.

Each sequence must be accompanied by a text file containing some captions with the
following format:

    caption#tokens#from_tag#to_tag

where `caption` describes one action in the sequence, `tokens` is the caption in a
tokenised form, and the part of the sequence described by the caption is delimited
by `from_tag` and `to_tag`. These last two values may be 0.0, in which case all of
the sequence is used. In the open data, mirrored captions are saved in files which
start with `M`:

    $ cat 003157.txt (truncated)
    a person makes a turn to the right.#a/DET person/NOUN make/VERB a/DET turn/VERB to/ADP the/DET right/NOUN#0.0#0.0
    $ cat M003157.txt (truncated)
    a person makes a turn to the left.#a/DET person/NOUN make/VERB a/DET turn/VERB to/ADP the/DET left/NOUN#0.0#0.0

We can use `annotate_texts.py` to annotate actions described in `$DATASET/texts/`.

Finally, we can compute the mean and variance arrays using `cal_mean_variance.py`,
adopted from the notebook of the same name.

[condpaper]:    https://arxiv.org/html/2405.11126v2#A1

Before training starts, `$DATASET` should have sub-directories `joints` and `vecs`
containing the raw and preprocessed sequences, and a corresponding `texts` holding
the descriptions of those actions.


## Training with the custom dataset

This is a summary of the steps to train on a custom rig called "myrig":

1. Copy the `data_loaders/custom` directory to `data_loaders/myrig`.
2. Update the dataset info for `myrig` in `data_loaders/myrig/data/dataset.py`.
3. Update `data_loaders/get_data.py`.
4. Update `data_loaders/myrig_utils.py`.
5. Update `utils/model_util.py`.
6. Update `utils/paramUtil.py`.
7. Update `utils/editing_util.py`.
8. Update `model/mdm_unet.py`.
9. Update `utils/get_opt`.
10. (Optional) Customize the training options in `configs/card.py`.
11. Now the training can be performed, e.g. to train for 1 million steps with a checkpoint every
   200K steps, run the following command:

```bash
python -m train.train_condmdi --dataset myrig --save_interval 200_000 --num_steps 1_000_000 --device 0 --keyframe_conditioned
```


## Doing inference with the custom dataset

In this case we are only handling conditional synthesis.

1. Update `sample/conditional_synthesis.py`.


The details of each step are highlighted below.


### Create a new data_loader class called `myrig`

Copy the `data_loaders/custom` directory to a new directory and call it `data_loaders/myrig`.


### Update dataset info for `myrig` in `data_loaders/myrig/data/dataset.py`

This file contains the specific settings for this rig.

  - create new subclass `myrig` from data.Dataset here with specific settings
    - /dataset/humanml_opt.txt is loaded as `opt` and `self.opt` within subclass
  - import necessary dependencies (ignore t2m?)
  - Text2MotionDatasetV2 and TextOnlyDataset depend on `--joints_num`, include that
  - train t2m for custom rig here (make sure your training data is longer than `min_motion_len`)
    - min_motion_len = 5 for t2m, else 24 (sequences below 5 frames are skipped)
  - update the feet and facing joints in `motion_to_rel_data` and `motion_to_abs_data`
    - start and end joints of left foot and right foot
    - facing joints are Z-shape: right hip, left hip, right shoulder, left shoulder
  - update the njoints in `sample_to_motion`, `abs3d_to_rel`, and `rel_to_abs3d`
    - 22 is the default value for the HumanML3D dataset.


### Update `data_loaders/get_data.py`

This file contains the list of classes which can be used to create the model and the diffusion,
both of which are used during training and inference. So we need to add `myrig` to the lists in
both `get_dataset_class` and `get_dataset`.

This means a new class called `myrig` needs to be built based on the default `HumanML3D` class.
We can use `CustomRig` as a template. Once that is done, import it in `get_dataset_class`.


### Update `scripts/motion_process.py`

Some convenience values are hardcoded at the top of this file. In future, we should import them
from `utils/paramUtil.py` directly, but refactoring will not be done just yet.

These are the leg joints `l_idx1` and `l_idx2` (which may be the same as hip joints `r_hip` and
`l_hip`), the right foot and left foot arrays `fid_r` and `fid_l`, the face joints vector which
is a 'Z' traced from the right hip to the left hip, to the right upper arm, then the left upper
arm, `face_joint_indx`, and `joints_num` the number of joints in the rig.

Also check that `custom_raw_offsets` and `custom_kinematic_chain` from `paramUtils` are used in
`process_file`. This function uses `data_dir` to generate `global_positions`, but retaining its
original value of `./dataset/000021.npy` somehow still allows custom rig training.

`recover_rot` hardcodes `joints_num` for HumanML3D and KIT only, but this function is currently
not in use and may be removed in the future. The same applies to the `main()` block which tries
to process KIT data.


### Update `data_loaders/myrig_utils.py`
The training process also requires some utilities that are currently only defined for HumanML3D
and AMASS classes. Make sure `data_loaders/myrig_utils.py` exists and has the updated values.

Here, the matrices for `myrig` are initialized to the same shape as the computed joint vectors,
so that they can be imported into `utils/editing_util.py` for `joint_to_full_mask_custom`.


### Update `utils/model_util.py`

Make sure `myrig` is imported and included in the list of `Datasets`.

Both training and inference require the `create_model_and_diffusion` function, so updating this
will help both sides of the process. Since we are just reusing the UNET architecture which they
built for the HumanML3D data, we don't need to change anything here.

However, it depends on `get_model_args` which requires the dataset name and number of `njoints`
corresponding to 12\*J - 1, where J is the actual number of joints in the rig as defined in the
appendix of [the paper][condpaper].

For example, with `humanml`, since the rig has 22 joints, `njoints` is set to 263.


### Update `utils/paramUtil.py`

  - create a `kinematic_chain` and `raw_offset` corresponding to `myrig`, for example:
       ```
          0
          |
       5--1--3
       |  |  |
       6  2  4
       ```

    - `myrig_kinematic_chain`: a list of joint chains, here: [[0, 1, 2], [1, 3, 4], [1, 5, 6]].
       Each sublist represents a single chain of joints (see above for explanation.)
    - `myrig_raw_offset`: numpy array of relative displacement of each node from its parent, in
      `[x,y,z]` order. In the above example, 0 is at the top, 3 and 5 are on the right and left
       of 1, and nodes 1, 2, 4, 6 are each below their parent. This gives us: 
            ```
            myrig_raw_offset = np.array(
              [
                [ 0, 0, 0],
                [ 0,-1, 0],
                [ 0,-1, 0],
                [ 1, 0, 0],
                [ 0,-1, 0],
                [-1, 0, 0],
                [ 0,-1, 0]
              ])
            ```

Include the above structures in `paramUtil.py` for the [HumanML3D][hml3d_fork] scripts, so that
the custom `myrig` can be preprocessed with the `build_vectors.py`.


### Update `utils/editing_util.py`

Make sure that the `from data_loaders` import line includes `myrig_utils`.

Then update the `joint_to_full_mask_custom` function with the correct import name. Ideally, the
`joint_to_full_mask` function should be refactored to take the rig type as an argument, but for
now we use a separate function.


### Update `model/mdm_unet.py`

Here, we have to manually add the details of `myrig` into the `MDM_UNET` class in a few places.

Since we want keyframe conditioning, add `myrig` in the if/elif block in the constructor as one
of the possible values for `self.dataset`. Set the value of `added_channels` to (12\*J - 1) for
the rig to be processed properly.

Since we want to use our text as well, add `myrig` to the `self.dataset` list in `encode_text`.
It is not yet clear how the maximum token length affects training, but we reuse the values from
the previous classes.

In `forward()` an `assert` is used to limit the rig types that can use this model. Add `myrig`.

Then make sure in `forward_core()` that the correct shape for `myrig` is used by adding `myrig`
and the corresponding number of `njoints` to the if/elif block for keyframe conditioning.


### Update `utils/get_opt.py`

The `get_opt()` function is used to read in training arguments from `./dataset/humanml_opt.txt`
which we can reuse. Note that `dataset_name` is set to `t2m` here and `max_text_len` is 20. The
`data_root` and `data_dir` options should be pointing to the trained vector data. Make sure the
values of `joints_num` and `dim_pose` are correctly defined. `dim_pose` should be computed from
the number of joints J to be (12\*J - 1). J is now read from `humanml_opt.txt`.

After running `annotate_texts.py`, point `text_dir` to the directory with the processed texts.


### (Optional) Customize the training options in `configs/card.py`.

If you need to permanently change some training options, you can create a new dataclass card in
`configs/card.py` subclassing the default data and model settings and giving the updated values
there, for instance to set `batch_size` to 2:

```python
@dataclass
class motion_abs_unet_adagn_xl_custom_batch(    # this class name should go to train_args()
    data.humanml_motion_abs,      # this is the current default for absolute motion data
    model.motion_unet_adagn_xl,   # this is the current default unet training setting
):
    batch_size: int = 2         ## change the batch size here
```
`utils/parser_util.py` contains all the options you can override this way.

Use the class name in `train_args(base_cls=$CARD_NAME)` before you run `train/train_condmdi.py`
if you use this method. To change settings one-off, just use command line options instead:

```bash
python -m train.train_condmdi --dataset myrig [... as above ...] --batch_size 2
```


## Using the trained custom model for inference

Currently, we only use conditional synthesis. Add `myrig` to `sample/conditional_synthesis.py`.
There is an `assert` in `main()` which tests for the model name and a place to specify `fps` or
`max_frames` for `myrig`.

Make sure that `args.dataset` checks for `myrig`. We can keep `hml_vec` for `model.data_rep` as
this just reshapes the input without further manipulation. However, further down in `main()` we
need to give the right number of `n_joints` for `myrig`.

If we want to generate `mp4` videos of the output as well, we can add `myrig` to the list where
we check `args.dataset` and run `plot_conditional_samples`. However, this is not needed for the
actual inference to run.

Once all this is done, we can run the conditional synthesis as shown below:

```bash
python -m sample.conditional_synthesis \
       --dataset="myrig" --model_path "./save/path/modelx.pt" --edit_mode benchmark_sparse \
       --transition_length 100 --n_keyframes 3 --num_repetitions 10 --seed 199 \
       --text_prompt "a man walks across the room, trips and stumbles, then squats down"
```

where `model_path` is the trained model checkpoint for this synthesis, `transition_length`
is the gap between each of the `n_keyframes` taken from a random sequence identified using
the `seed` value we set (here, 199) and `num_repetitions` is the number of trial sequences
to generate using the `text_prompt` provided.

To vary the strength of the reference action vs the text prompt, experiment with the value
of `transition_length` and maybe consider setting `keyframe_guidance_param` (default value
is 1) to above 2.5 as suggested [in this bug thread][sparse].

[sparse]: https://github.com/setarehc/diffusion-motion-inbetweening/issues/5#issuecomment-2197243178


## Producing Output

The result of the synthesis is a numpy array, `results.npy`, which contains the sequences.
Each of these sequences can be converted into BVH format using the [joints2bvh][momjoints]
script from the momask project, which other tools can convert to formats such as FBX.

- can the existing scripts convert arbitrary J-joint rigs to the correct form? (no, need to fix)
- will need to update momask joints2bvh: convert() to use nonstandard rig as well

----
> end of document
----

# Working Notes to Explore


## Get_Data.py
- dataset:
  - add new type in DataOptions
  - where is keyframe_conditioned used in ModelOptions?
  - get_data.py:
    - get_dataset_class("classname")
    - from data_loaders.CLASSNAME.data import CLASSNAME

## From the Training Arguments dataclasses
args = train_args(base_cls=card.motion_abs_unet_adagn_xl)
     = HfArgumentParser(base_cls).parse_args_into_dataclasses()[0]

--> does the base_cls affect any params?

--> TrainArgs(BaseOptions, DataOptions, ModelOptions, DiffusionOptions, TrainingOptions)
    - cuda: bool=True
    - device: int=0
    - seed: int=10

    - dataset: str="humanml", ["humanml", "kit", "humanact12", "uestc", "amass"]
    - data_dir: str="", check dataset defaults
    - abs_3d: bool=False
    - traj_only: bool=False
    - xz_only: Tuple[int]=False??
    - use_random_proj: bool=False
    - random_proj_scale: float=10.0
    - augment_type: str="none", ["none", "rot", "full"]
    - std_scale_shift: Tuple[float]=(1.0, 0.0)
    - drop_redundant: bool=False (if true, keep only 4 + 21*3)

    - arch: str="trans_enc", check paper for arch types
    - emb_trans_dec: bool=False (toggle inject condition as class token in trans_dec)
    - layers: int=8
    - latent_dim: int=512 (tf/gru width)
    - ff_size: int=1024 (tf feedforward size)
    - dim_mults: Tuple[float]=(2, 2, 2, 2) (channel multipliers for unet)
    - unet_adagn: bool=True (adaptive group normalization for unet)
    - unet_zero: bool=True (zero weight init for unet)
    - out_mult: bool=1 (large variation feature multiplier for unet/tf)
    - cond_mask_prob: float=0.1 (prob(mask cond during training) for cfg)
    - keyframe_mask_prob: float=0.1 (prob(mask keyframe cond during training) for cfg)
    - lambda_rcxyz: float=0.0, joint pos loss
    - lambda_vel: float=0.0, joint vel loss
    - lambda_fc: float=0.0, foot contact loss
    - unconstrained: bool=False (training independent of text, action. only humanact12)
    - keyframe_conditioned: bool=False (condition on keyframes. only hml3d)
    - keyframe_selection_scheme: str="random_frames", ["random_frames", "random_joints", "random"]
    - zero_keyframe_loss: bool=False (zero the loss over observed keyframe loss, or allow model to make predictions over observed keyframes if false)

    - noise_schedule: str="cosine"
    - diffusion_steps: int=1000, T in paper
    - sigma_small: bool=True, what?
    - predict_xstart: bool=True, what?
    - use_ddim: bool=False, what?
    - clip_range: float=6.0, range to clip what?

    - save_dir: str=None
    - overwrite: bool=False, true to reuse existing dir
    - batch_size: int=64
    - train_platform_type: str="NoPlatform", ["NoPlatform", "ClearmlPlatform", "TensorboardPlatform", "WandbPlatform"]
    - lr: float=1e-4, learning rate
    - weight_decay: float=0, optimizer weight decay
    - grad_clip: float=0, gradient clip
    - use_fp16: bool=False
    - avg_model_beta: float=0, 0 means disabled
    - adam_beta2: float=0.999
    - lr_anneal_steps: int=0
    - eval_batch_size: int=32, <<do not touch warning>>
    - eval_split: str="test", ["val", "test"]
    - eval_during_training: bool=False
    - eval_rep_times: int=3, times to loop evaluation during training
    - eval_num_samples: int=1_000, set to -1 to use all
    - log_interval: int=1_000, N steps before losses should be logged
    - save_interval: int=100_000, N steps to save checkpoint AND run evaluation if asked
    - num_steps: int=1_200_000
    - num_frames: int=60, frame limit ignored by hml3d and KIT (check what the value there is)
    - resume_checkpoint: str="", continue training from checkpoint 'model_.pt'
    - apply_zero_mask: bool=False
    - traj_extra_weight: float=1.0, extra weight for what?
    - time_weighted_loss: bool=False, what does this do?
    - train_x0_as_eps: bool=False, what is x0 and what is eps?
